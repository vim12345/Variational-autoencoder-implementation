{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§ª Python Implementation (Variational Autoencoder):\n"
      ],
      "metadata": {
        "id": "C9XeNx8fp_yc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GgFzxVKgp-Tb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the VAE architecture\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, z_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, z_dim)  # Mean of z\n",
        "        self.fc22 = nn.Linear(400, z_dim)  # Log-variance of z\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(z_dim, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = torch.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        ""
      ],
      "metadata": {
        "id": "IUK38RuJqDAS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Loss function for VAE\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.BCELoss(reduction='sum')(recon_x, x.view(-1, 784))\n",
        "    # KL divergence between the learned distribution and the prior\n",
        "    # Regularizing the latent space\n",
        "    # D_KL = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    # where mu, sigma are the learned mean and standard deviation of the latent variable\n",
        "    # This encourages the learned distribution to be close to the standard normal distribution\n",
        "    # for smooth and consistent sampling.\n",
        "    # In simple terms, it penalizes the encoder from outputting too large/small of variance\n",
        "    # or high deviations in latent space.\n",
        "    # It ensures the latent space stays close to Gaussian distribution.\n",
        "    # More about variational inference here: https://arxiv.org/abs/1312.6114\n",
        "    # https://dl.acm.org/doi/10.1145/3065386\n",
        "    # https://www.jmlr.org/papers/volume15/kingma14a/kingma14a.pdf\n",
        "    # reference: https://pytorch.org/tutorials/beginner/vae.html\n",
        "    # We are calculating the Kullback-Leibler divergence between the learned distribution\n",
        "    # (q(z|x)) and a prior distribution (usually chosen as N(0,I))\n",
        "    # Note that the prior is not trainable.\n",
        "    # The learned distribution q(z|x) comes from the encoder, so its parameters (mean, variance)\n",
        "    # depend on the input data.\n",
        "    # The prior on the other hand, is fixed and is independent of x.\n",
        "    # The code implements this via logvar (log variance).\n",
        "    # z ~ N(0, I)\n",
        "    # Both mu (mean) and logvar are the outputs of the encoder network. We can think of this as variational inference.\n",
        "    # In practice, we use the reparameterization trick (sampling z from N(0, I)).\n",
        "    # D_KL divergence encourages the encoder to output a distribution close to N(0, I).\n",
        "    # This is added to the reconstruction error (BCE loss) during optimization.\n",
        "    # Overall loss = Reconstruction loss + KL Divergence loss (regularization term).\n",
        "    # Note that the BCE term tries to match the distribution to the data and the KL term tries to keep it close to N(0, I).\n",
        "    # This means that the model will learn how to generate new, similar data while keeping the latent space organized.\n",
        "\n",
        "    # KL divergence term\n",
        "    # Equation for KL divergence\n",
        "    # eq. (9) in Kingma et al.\n",
        "    # D_KL = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    # print(f\"BCE: {BCE:.4f}, KL: {KL:.4f}\")\n",
        "    # KL divergence loss\n",
        "    # sum(log(sigma^2) - mu^2 - sigma^2 + 1)\n",
        "    # mu and logvar are outputs of encoder\n",
        "    # note, using the Gaussian prior\n",
        "    # q(z|x) is the encoder, p(z) is the prior, and D_KL is the divergence between the two\n",
        "    # Tying the encoder's output to the prior by minimizing the KL divergence term\n",
        "    # that ensures the learned distribution stays close to the prior\n",
        "    # we want the decoder to be able to generate from the learned latent distribution\n",
        "\n",
        "    # sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    # Returning loss that combines reconstruction and regularization\n",
        "    # KLD term (KL divergence term is penalized)\n",
        "    # full loss:\n",
        "    # L_total = L_reconstruction + L_kl\n",
        "    # See also Eq. (12) in Kingma et al.\n",
        "    # ref: https://pytorch.org/tutorials/beginner/vae.html\n",
        "    # more on VAE from https://arxiv.org/abs/1312.6114\n",
        "    # see equation (2) of Kingma et al. for reconstruction and regularization terms\n",
        "    # more about VAEs: https://arxiv.org/abs/1907.09711\n",
        "    # finally: loss function = BCE + KL divergence\n",
        "    # This produces the final sum\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        ""
      ],
      "metadata": {
        "id": "gGOoabrzqFja"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Set up training loop\n",
        "def train_vae():\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    # Initialize model, optimizer\n",
        "    model = VAE()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss = loss_function(recon_batch, data, mu, logvar)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss / len(train_loader.dataset):.4f}\")\n",
        ""
      ],
      "metadata": {
        "id": "zgLKwKJnqJQ0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the VAE\n",
        "train_vae()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhXfkPQLqN-Z",
        "outputId": "000f260c-fa2c-48ae-a851-99b0b9ae9256"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 54.8MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 1.67MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 12.5MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.32MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 164.6890\n",
            "Epoch 2/10, Loss: 121.3300\n",
            "Epoch 3/10, Loss: 114.4451\n",
            "Epoch 4/10, Loss: 111.4408\n",
            "Epoch 5/10, Loss: 109.6873\n",
            "Epoch 6/10, Loss: 108.5152\n",
            "Epoch 7/10, Loss: 107.7563\n",
            "Epoch 8/10, Loss: 107.1066\n",
            "Epoch 9/10, Loss: 106.5642\n",
            "Epoch 10/10, Loss: 106.1789\n"
          ]
        }
      ]
    }
  ]
}